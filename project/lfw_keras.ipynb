{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n",
      "(1288, 131, 98)\n",
      "Total dataset size:\n",
      "n_samples: 1288\n",
      "n_features: 12838\n",
      "n_classes: 7\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from time import time\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.decomposition import PCA as RandomizedPCA\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Download the data, if not already on disk and load it as numpy arrays\n",
    "\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=1.05)\n",
    "\n",
    "# introspect the images arrays to find the shapes (for plotting)\n",
    "n_samples, h, w = lfw_people.images.shape\n",
    "\n",
    "print(lfw_people.images.shape)\n",
    "# for machine learning we use the 2 data directly (as relative pixel\n",
    "# positions info is ignored by this model)\n",
    "X = lfw_people.data\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# the label to predict is the id of the person\n",
    "y = lfw_people.target\n",
    "\n",
    "target_names = lfw_people.target_names\n",
    "n_classes = target_names.shape[0]\n",
    "\n",
    "print(\"Total dataset size:\")\n",
    "print(\"n_samples: %d\" % n_samples)\n",
    "print(\"n_features: %d\" % n_features)\n",
    "print(\"n_classes: %d\" % n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split into a training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(1288, 131, 98)\n",
    "X = np.stack((X,)*3, axis=-1)\n",
    "y = to_categorical(y, num_classes=n_classes)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(966, 131, 98, 3)\n",
      "(966, 7)\n",
      "(322, 131, 98, 3)\n",
      "(322, 7)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute a PCA (eigenfaces) on the face dataset (treated as unlabeled\n",
    "dataset): unsupervised feature extraction / dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_components = 150\n",
    "\n",
    "# print(\"Extracting the top %d eigenfaces from %d faces\"\n",
    "#       % (n_components, X_train.shape[0]))\n",
    "# t0 = time()\n",
    "# pca = RandomizedPCA(n_components=n_components, whiten=True).fit(X_train)\n",
    "# print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "# eigenfaces = pca.components_.reshape((n_components, h, w, 1))\n",
    "# print(eigenfaces.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "30/30 [==============================] - 320s 11s/step - loss: 1.7194\n",
      "Epoch 2/5\n",
      "30/30 [==============================] - 245s 8s/step - loss: 0.7504\n",
      "Epoch 3/5\n",
      "30/30 [==============================] - 244s 8s/step - loss: 0.3958\n",
      "Epoch 4/5\n",
      "30/30 [==============================] - 255s 8s/step - loss: 0.2355\n",
      "Epoch 5/5\n",
      "30/30 [==============================] - 246s 8s/step - loss: 0.1926\n",
      "0 input_26\n",
      "1 conv2d_2223\n",
      "2 batch_normalization_2223\n",
      "3 activation_2221\n",
      "4 conv2d_2224\n",
      "5 batch_normalization_2224\n",
      "6 activation_2222\n",
      "7 conv2d_2225\n",
      "8 batch_normalization_2225\n",
      "9 activation_2223\n",
      "10 max_pooling2d_98\n",
      "11 conv2d_2226\n",
      "12 batch_normalization_2226\n",
      "13 activation_2224\n",
      "14 conv2d_2227\n",
      "15 batch_normalization_2227\n",
      "16 activation_2225\n",
      "17 max_pooling2d_99\n",
      "18 conv2d_2231\n",
      "19 batch_normalization_2231\n",
      "20 activation_2229\n",
      "21 conv2d_2229\n",
      "22 conv2d_2232\n",
      "23 batch_normalization_2229\n",
      "24 batch_normalization_2232\n",
      "25 activation_2227\n",
      "26 activation_2230\n",
      "27 average_pooling2d_214\n",
      "28 conv2d_2228\n",
      "29 conv2d_2230\n",
      "30 conv2d_2233\n",
      "31 conv2d_2234\n",
      "32 batch_normalization_2228\n",
      "33 batch_normalization_2230\n",
      "34 batch_normalization_2233\n",
      "35 batch_normalization_2234\n",
      "36 activation_2226\n",
      "37 activation_2228\n",
      "38 activation_2231\n",
      "39 activation_2232\n",
      "40 mixed0\n",
      "41 conv2d_2238\n",
      "42 batch_normalization_2238\n",
      "43 activation_2236\n",
      "44 conv2d_2236\n",
      "45 conv2d_2239\n",
      "46 batch_normalization_2236\n",
      "47 batch_normalization_2239\n",
      "48 activation_2234\n",
      "49 activation_2237\n",
      "50 average_pooling2d_215\n",
      "51 conv2d_2235\n",
      "52 conv2d_2237\n",
      "53 conv2d_2240\n",
      "54 conv2d_2241\n",
      "55 batch_normalization_2235\n",
      "56 batch_normalization_2237\n",
      "57 batch_normalization_2240\n",
      "58 batch_normalization_2241\n",
      "59 activation_2233\n",
      "60 activation_2235\n",
      "61 activation_2238\n",
      "62 activation_2239\n",
      "63 mixed1\n",
      "64 conv2d_2245\n",
      "65 batch_normalization_2245\n",
      "66 activation_2243\n",
      "67 conv2d_2243\n",
      "68 conv2d_2246\n",
      "69 batch_normalization_2243\n",
      "70 batch_normalization_2246\n",
      "71 activation_2241\n",
      "72 activation_2244\n",
      "73 average_pooling2d_216\n",
      "74 conv2d_2242\n",
      "75 conv2d_2244\n",
      "76 conv2d_2247\n",
      "77 conv2d_2248\n",
      "78 batch_normalization_2242\n",
      "79 batch_normalization_2244\n",
      "80 batch_normalization_2247\n",
      "81 batch_normalization_2248\n",
      "82 activation_2240\n",
      "83 activation_2242\n",
      "84 activation_2245\n",
      "85 activation_2246\n",
      "86 mixed2\n",
      "87 conv2d_2250\n",
      "88 batch_normalization_2250\n",
      "89 activation_2248\n",
      "90 conv2d_2251\n",
      "91 batch_normalization_2251\n",
      "92 activation_2249\n",
      "93 conv2d_2249\n",
      "94 conv2d_2252\n",
      "95 batch_normalization_2249\n",
      "96 batch_normalization_2252\n",
      "97 activation_2247\n",
      "98 activation_2250\n",
      "99 max_pooling2d_100\n",
      "100 mixed3\n",
      "101 conv2d_2257\n",
      "102 batch_normalization_2257\n",
      "103 activation_2255\n",
      "104 conv2d_2258\n",
      "105 batch_normalization_2258\n",
      "106 activation_2256\n",
      "107 conv2d_2254\n",
      "108 conv2d_2259\n",
      "109 batch_normalization_2254\n",
      "110 batch_normalization_2259\n",
      "111 activation_2252\n",
      "112 activation_2257\n",
      "113 conv2d_2255\n",
      "114 conv2d_2260\n",
      "115 batch_normalization_2255\n",
      "116 batch_normalization_2260\n",
      "117 activation_2253\n",
      "118 activation_2258\n",
      "119 average_pooling2d_217\n",
      "120 conv2d_2253\n",
      "121 conv2d_2256\n",
      "122 conv2d_2261\n",
      "123 conv2d_2262\n",
      "124 batch_normalization_2253\n",
      "125 batch_normalization_2256\n",
      "126 batch_normalization_2261\n",
      "127 batch_normalization_2262\n",
      "128 activation_2251\n",
      "129 activation_2254\n",
      "130 activation_2259\n",
      "131 activation_2260\n",
      "132 mixed4\n",
      "133 conv2d_2267\n",
      "134 batch_normalization_2267\n",
      "135 activation_2265\n",
      "136 conv2d_2268\n",
      "137 batch_normalization_2268\n",
      "138 activation_2266\n",
      "139 conv2d_2264\n",
      "140 conv2d_2269\n",
      "141 batch_normalization_2264\n",
      "142 batch_normalization_2269\n",
      "143 activation_2262\n",
      "144 activation_2267\n",
      "145 conv2d_2265\n",
      "146 conv2d_2270\n",
      "147 batch_normalization_2265\n",
      "148 batch_normalization_2270\n",
      "149 activation_2263\n",
      "150 activation_2268\n",
      "151 average_pooling2d_218\n",
      "152 conv2d_2263\n",
      "153 conv2d_2266\n",
      "154 conv2d_2271\n",
      "155 conv2d_2272\n",
      "156 batch_normalization_2263\n",
      "157 batch_normalization_2266\n",
      "158 batch_normalization_2271\n",
      "159 batch_normalization_2272\n",
      "160 activation_2261\n",
      "161 activation_2264\n",
      "162 activation_2269\n",
      "163 activation_2270\n",
      "164 mixed5\n",
      "165 conv2d_2277\n",
      "166 batch_normalization_2277\n",
      "167 activation_2275\n",
      "168 conv2d_2278\n",
      "169 batch_normalization_2278\n",
      "170 activation_2276\n",
      "171 conv2d_2274\n",
      "172 conv2d_2279\n",
      "173 batch_normalization_2274\n",
      "174 batch_normalization_2279\n",
      "175 activation_2272\n",
      "176 activation_2277\n",
      "177 conv2d_2275\n",
      "178 conv2d_2280\n",
      "179 batch_normalization_2275\n",
      "180 batch_normalization_2280\n",
      "181 activation_2273\n",
      "182 activation_2278\n",
      "183 average_pooling2d_219\n",
      "184 conv2d_2273\n",
      "185 conv2d_2276\n",
      "186 conv2d_2281\n",
      "187 conv2d_2282\n",
      "188 batch_normalization_2273\n",
      "189 batch_normalization_2276\n",
      "190 batch_normalization_2281\n",
      "191 batch_normalization_2282\n",
      "192 activation_2271\n",
      "193 activation_2274\n",
      "194 activation_2279\n",
      "195 activation_2280\n",
      "196 mixed6\n",
      "197 conv2d_2287\n",
      "198 batch_normalization_2287\n",
      "199 activation_2285\n",
      "200 conv2d_2288\n",
      "201 batch_normalization_2288\n",
      "202 activation_2286\n",
      "203 conv2d_2284\n",
      "204 conv2d_2289\n",
      "205 batch_normalization_2284\n",
      "206 batch_normalization_2289\n",
      "207 activation_2282\n",
      "208 activation_2287\n",
      "209 conv2d_2285\n",
      "210 conv2d_2290\n",
      "211 batch_normalization_2285\n",
      "212 batch_normalization_2290\n",
      "213 activation_2283\n",
      "214 activation_2288\n",
      "215 average_pooling2d_220\n",
      "216 conv2d_2283\n",
      "217 conv2d_2286\n",
      "218 conv2d_2291\n",
      "219 conv2d_2292\n",
      "220 batch_normalization_2283\n",
      "221 batch_normalization_2286\n",
      "222 batch_normalization_2291\n",
      "223 batch_normalization_2292\n",
      "224 activation_2281\n",
      "225 activation_2284\n",
      "226 activation_2289\n",
      "227 activation_2290\n",
      "228 mixed7\n",
      "229 conv2d_2295\n",
      "230 batch_normalization_2295\n",
      "231 activation_2293\n",
      "232 conv2d_2296\n",
      "233 batch_normalization_2296\n",
      "234 activation_2294\n",
      "235 conv2d_2293\n",
      "236 conv2d_2297\n",
      "237 batch_normalization_2293\n",
      "238 batch_normalization_2297\n",
      "239 activation_2291\n",
      "240 activation_2295\n",
      "241 conv2d_2294\n",
      "242 conv2d_2298\n",
      "243 batch_normalization_2294\n",
      "244 batch_normalization_2298\n",
      "245 activation_2292\n",
      "246 activation_2296\n",
      "247 max_pooling2d_101\n",
      "248 mixed8\n",
      "249 conv2d_2303\n",
      "250 batch_normalization_2303\n",
      "251 activation_2301\n",
      "252 conv2d_2300\n",
      "253 conv2d_2304\n",
      "254 batch_normalization_2300\n",
      "255 batch_normalization_2304\n",
      "256 activation_2298\n",
      "257 activation_2302\n",
      "258 conv2d_2301\n",
      "259 conv2d_2302\n",
      "260 conv2d_2305\n",
      "261 conv2d_2306\n",
      "262 average_pooling2d_221\n",
      "263 conv2d_2299\n",
      "264 batch_normalization_2301\n",
      "265 batch_normalization_2302\n",
      "266 batch_normalization_2305\n",
      "267 batch_normalization_2306\n",
      "268 conv2d_2307\n",
      "269 batch_normalization_2299\n",
      "270 activation_2299\n",
      "271 activation_2300\n",
      "272 activation_2303\n",
      "273 activation_2304\n",
      "274 batch_normalization_2307\n",
      "275 activation_2297\n",
      "276 mixed9_0\n",
      "277 concatenate_47\n",
      "278 activation_2305\n",
      "279 mixed9\n",
      "280 conv2d_2312\n",
      "281 batch_normalization_2312\n",
      "282 activation_2310\n",
      "283 conv2d_2309\n",
      "284 conv2d_2313\n",
      "285 batch_normalization_2309\n",
      "286 batch_normalization_2313\n",
      "287 activation_2307\n",
      "288 activation_2311\n",
      "289 conv2d_2310\n",
      "290 conv2d_2311\n",
      "291 conv2d_2314\n",
      "292 conv2d_2315\n",
      "293 average_pooling2d_222\n",
      "294 conv2d_2308\n",
      "295 batch_normalization_2310\n",
      "296 batch_normalization_2311\n",
      "297 batch_normalization_2314\n",
      "298 batch_normalization_2315\n",
      "299 conv2d_2316\n",
      "300 batch_normalization_2308\n",
      "301 activation_2308\n",
      "302 activation_2309\n",
      "303 activation_2312\n",
      "304 activation_2313\n",
      "305 batch_normalization_2316\n",
      "306 activation_2306\n",
      "307 mixed9_1\n",
      "308 concatenate_48\n",
      "309 activation_2314\n",
      "310 mixed10\n",
      "Epoch 1/5\n",
      "30/30 [==============================] - 441s 15s/step - loss: 0.2589\n",
      "Epoch 2/5\n",
      " 8/30 [=======>......................] - ETA: 4:28 - loss: 0.1106"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-118-af380ee9d4ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# we train our model again (this time fine-tuning the top 2 inception blocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# alongside the top Dense layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mBS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/bff3/cs344/venv/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/home/bff3/cs344/venv/lib/python3.5/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bff3/cs344/venv/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bff3/cs344/venv/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bff3/cs344/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# create the base pre-trained model\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(131,98,3))\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "# and a logistic layer -- let's say we have 200 classes\n",
    "predictions = Dense(7, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "EPOCHS = 5\n",
    "BS = 32\n",
    "\n",
    "# train the model on the new data for a few epochs\n",
    "model.fit(X_train, y_train, batch_size=None, steps_per_epoch=(len(X_train) // BS), epochs=EPOCHS)\n",
    "\n",
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from inception V3. We will freeze the bottom N layers\n",
    "# and train the remaining top layers.\n",
    "\n",
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "   print(i, layer.name)\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in model.layers[:249]:\n",
    "   layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "   layer.trainable = True\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "from keras.optimizers import SGD\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy')\n",
    "\n",
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "model.fit(X_train, y_train, batch_size=None, steps_per_epoch=(len(X_train) // BS), epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
