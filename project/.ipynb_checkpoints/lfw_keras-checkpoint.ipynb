{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n",
      "(1288, 125, 94)\n",
      "Total dataset size:\n",
      "n_samples: 1288\n",
      "n_features: 125\n",
      "n_classes: 7\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from time import time\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage import color\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.decomposition import PCA as RandomizedPCA\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import optimizers\n",
    "\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Download the data, if not already on disk and load it as numpy arrays\n",
    "\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=1)\n",
    "\n",
    "# introspect the images arrays to find the shapes (for plotting)\n",
    "n_samples, h, w = lfw_people.images.shape\n",
    "\n",
    "print(lfw_people.images.shape)\n",
    "# for machine learning we use the 2 data directly (as relative pixel\n",
    "# positions info is ignored by this model)\n",
    "X = lfw_people.images\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# the label to predict is the id of the person\n",
    "y = lfw_people.target\n",
    "\n",
    "target_names = lfw_people.target_names\n",
    "n_classes = target_names.shape[0]\n",
    "print(\"Total dataset size:\")\n",
    "print(\"n_samples: %d\" % n_samples)\n",
    "print(\"n_features: %d\" % n_features)\n",
    "print(\"n_classes: %d\" % n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "some preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "X_3chan = color.grey2rgb(X)\n",
    "y = to_categorical(y, num_classes=n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Split into test and training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(966, 125, 94, 3)\n",
      "(966, 7)\n",
      "(322, 125, 94, 3)\n",
      "(322, 7)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_3chan, y, test_size=0.25)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Initialize base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.7/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "219062272/219055592 [==============================] - 31s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "#base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(h,w,3))\n",
    "base_model = InceptionResNetV2(weights='imagenet', include_top=False, input_shape=(h,w,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "make model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "31/30 [==============================] - 29s 951ms/step - loss: 2.3782 - acc: 0.3644\n",
      "Epoch 2/15\n",
      "31/30 [==============================] - 17s 546ms/step - loss: 1.5873 - acc: 0.4804\n",
      "Epoch 3/15\n",
      "31/30 [==============================] - 16s 525ms/step - loss: 1.4532 - acc: 0.5079\n",
      "Epoch 4/15\n",
      "31/30 [==============================] - 16s 525ms/step - loss: 1.2868 - acc: 0.5676\n",
      "Epoch 5/15\n",
      "31/30 [==============================] - 16s 525ms/step - loss: 1.2212 - acc: 0.6116\n",
      "Epoch 6/15\n",
      "31/30 [==============================] - 16s 526ms/step - loss: 1.1477 - acc: 0.6029\n",
      "Epoch 7/15\n",
      "31/30 [==============================] - 16s 524ms/step - loss: 1.1791 - acc: 0.6192\n",
      "Epoch 8/15\n",
      "31/30 [==============================] - 17s 533ms/step - loss: 1.1290 - acc: 0.6293\n",
      "Epoch 9/15\n",
      "31/30 [==============================] - 16s 529ms/step - loss: 1.0917 - acc: 0.6221\n",
      "Epoch 10/15\n",
      "31/30 [==============================] - 16s 531ms/step - loss: 1.1053 - acc: 0.6261\n",
      "Epoch 11/15\n",
      "31/30 [==============================] - 17s 536ms/step - loss: 1.1520 - acc: 0.6300\n",
      "Epoch 12/15\n",
      "31/30 [==============================] - 17s 538ms/step - loss: 1.0791 - acc: 0.6433\n",
      "Epoch 13/15\n",
      "31/30 [==============================] - 17s 541ms/step - loss: 1.0469 - acc: 0.6788\n",
      "Epoch 14/15\n",
      "31/30 [==============================] - 17s 534ms/step - loss: 0.9639 - acc: 0.6990\n",
      "Epoch 15/15\n",
      "31/30 [==============================] - 16s 528ms/step - loss: 1.0674 - acc: 0.6624\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f12510c5cc0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "\n",
    "# and a logistic layer\n",
    "predictions = Dense(n_classes, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "BS = 32\n",
    "EPOCHS = 5\n",
    "\n",
    "# datagen = ImageDataGenerator(\n",
    "#     featurewise_center=True,\n",
    "#     featurewise_std_normalization=True,\n",
    "#     rotation_range=20,\n",
    "#     width_shift_range=0.2,\n",
    "#     height_shift_range=0.2,\n",
    "#     horizontal_flip=True)\n",
    "\n",
    "# compute quantities required for featurewise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied)\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# fits the model on batches with real-time data augmentation:\n",
    "# model.fit_generator(datagen.flow(X_train, y_train, batch_size=BS),\n",
    "#                     steps_per_epoch=len(X_train) / BS, epochs=EPOCHS)\n",
    "\n",
    "# train the model on the new data for a few epochs\n",
    "model.fit(X_train, y_train, batch_size=BS, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_5\n",
      "1 conv2d_377\n",
      "2 batch_normalization_377\n",
      "3 activation_377\n",
      "4 conv2d_378\n",
      "5 batch_normalization_378\n",
      "6 activation_378\n",
      "7 conv2d_379\n",
      "8 batch_normalization_379\n",
      "9 activation_379\n",
      "10 max_pooling2d_17\n",
      "11 conv2d_380\n",
      "12 batch_normalization_380\n",
      "13 activation_380\n",
      "14 conv2d_381\n",
      "15 batch_normalization_381\n",
      "16 activation_381\n",
      "17 max_pooling2d_18\n",
      "18 conv2d_385\n",
      "19 batch_normalization_385\n",
      "20 activation_385\n",
      "21 conv2d_383\n",
      "22 conv2d_386\n",
      "23 batch_normalization_383\n",
      "24 batch_normalization_386\n",
      "25 activation_383\n",
      "26 activation_386\n",
      "27 average_pooling2d_37\n",
      "28 conv2d_382\n",
      "29 conv2d_384\n",
      "30 conv2d_387\n",
      "31 conv2d_388\n",
      "32 batch_normalization_382\n",
      "33 batch_normalization_384\n",
      "34 batch_normalization_387\n",
      "35 batch_normalization_388\n",
      "36 activation_382\n",
      "37 activation_384\n",
      "38 activation_387\n",
      "39 activation_388\n",
      "40 mixed_5b\n",
      "41 conv2d_392\n",
      "42 batch_normalization_392\n",
      "43 activation_392\n",
      "44 conv2d_390\n",
      "45 conv2d_393\n",
      "46 batch_normalization_390\n",
      "47 batch_normalization_393\n",
      "48 activation_390\n",
      "49 activation_393\n",
      "50 conv2d_389\n",
      "51 conv2d_391\n",
      "52 conv2d_394\n",
      "53 batch_normalization_389\n",
      "54 batch_normalization_391\n",
      "55 batch_normalization_394\n",
      "56 activation_389\n",
      "57 activation_391\n",
      "58 activation_394\n",
      "59 block35_1_mixed\n",
      "60 block35_1_conv\n",
      "61 block35_1\n",
      "62 block35_1_ac\n",
      "63 conv2d_398\n",
      "64 batch_normalization_398\n",
      "65 activation_398\n",
      "66 conv2d_396\n",
      "67 conv2d_399\n",
      "68 batch_normalization_396\n",
      "69 batch_normalization_399\n",
      "70 activation_396\n",
      "71 activation_399\n",
      "72 conv2d_395\n",
      "73 conv2d_397\n",
      "74 conv2d_400\n",
      "75 batch_normalization_395\n",
      "76 batch_normalization_397\n",
      "77 batch_normalization_400\n",
      "78 activation_395\n",
      "79 activation_397\n",
      "80 activation_400\n",
      "81 block35_2_mixed\n",
      "82 block35_2_conv\n",
      "83 block35_2\n",
      "84 block35_2_ac\n",
      "85 conv2d_404\n",
      "86 batch_normalization_404\n",
      "87 activation_404\n",
      "88 conv2d_402\n",
      "89 conv2d_405\n",
      "90 batch_normalization_402\n",
      "91 batch_normalization_405\n",
      "92 activation_402\n",
      "93 activation_405\n",
      "94 conv2d_401\n",
      "95 conv2d_403\n",
      "96 conv2d_406\n",
      "97 batch_normalization_401\n",
      "98 batch_normalization_403\n",
      "99 batch_normalization_406\n",
      "100 activation_401\n",
      "101 activation_403\n",
      "102 activation_406\n",
      "103 block35_3_mixed\n",
      "104 block35_3_conv\n",
      "105 block35_3\n",
      "106 block35_3_ac\n",
      "107 conv2d_410\n",
      "108 batch_normalization_410\n",
      "109 activation_410\n",
      "110 conv2d_408\n",
      "111 conv2d_411\n",
      "112 batch_normalization_408\n",
      "113 batch_normalization_411\n",
      "114 activation_408\n",
      "115 activation_411\n",
      "116 conv2d_407\n",
      "117 conv2d_409\n",
      "118 conv2d_412\n",
      "119 batch_normalization_407\n",
      "120 batch_normalization_409\n",
      "121 batch_normalization_412\n",
      "122 activation_407\n",
      "123 activation_409\n",
      "124 activation_412\n",
      "125 block35_4_mixed\n",
      "126 block35_4_conv\n",
      "127 block35_4\n",
      "128 block35_4_ac\n",
      "129 conv2d_416\n",
      "130 batch_normalization_416\n",
      "131 activation_416\n",
      "132 conv2d_414\n",
      "133 conv2d_417\n",
      "134 batch_normalization_414\n",
      "135 batch_normalization_417\n",
      "136 activation_414\n",
      "137 activation_417\n",
      "138 conv2d_413\n",
      "139 conv2d_415\n",
      "140 conv2d_418\n",
      "141 batch_normalization_413\n",
      "142 batch_normalization_415\n",
      "143 batch_normalization_418\n",
      "144 activation_413\n",
      "145 activation_415\n",
      "146 activation_418\n",
      "147 block35_5_mixed\n",
      "148 block35_5_conv\n",
      "149 block35_5\n",
      "150 block35_5_ac\n",
      "151 conv2d_422\n",
      "152 batch_normalization_422\n",
      "153 activation_422\n",
      "154 conv2d_420\n",
      "155 conv2d_423\n",
      "156 batch_normalization_420\n",
      "157 batch_normalization_423\n",
      "158 activation_420\n",
      "159 activation_423\n",
      "160 conv2d_419\n",
      "161 conv2d_421\n",
      "162 conv2d_424\n",
      "163 batch_normalization_419\n",
      "164 batch_normalization_421\n",
      "165 batch_normalization_424\n",
      "166 activation_419\n",
      "167 activation_421\n",
      "168 activation_424\n",
      "169 block35_6_mixed\n",
      "170 block35_6_conv\n",
      "171 block35_6\n",
      "172 block35_6_ac\n",
      "173 conv2d_428\n",
      "174 batch_normalization_428\n",
      "175 activation_428\n",
      "176 conv2d_426\n",
      "177 conv2d_429\n",
      "178 batch_normalization_426\n",
      "179 batch_normalization_429\n",
      "180 activation_426\n",
      "181 activation_429\n",
      "182 conv2d_425\n",
      "183 conv2d_427\n",
      "184 conv2d_430\n",
      "185 batch_normalization_425\n",
      "186 batch_normalization_427\n",
      "187 batch_normalization_430\n",
      "188 activation_425\n",
      "189 activation_427\n",
      "190 activation_430\n",
      "191 block35_7_mixed\n",
      "192 block35_7_conv\n",
      "193 block35_7\n",
      "194 block35_7_ac\n",
      "195 conv2d_434\n",
      "196 batch_normalization_434\n",
      "197 activation_434\n",
      "198 conv2d_432\n",
      "199 conv2d_435\n",
      "200 batch_normalization_432\n",
      "201 batch_normalization_435\n",
      "202 activation_432\n",
      "203 activation_435\n",
      "204 conv2d_431\n",
      "205 conv2d_433\n",
      "206 conv2d_436\n",
      "207 batch_normalization_431\n",
      "208 batch_normalization_433\n",
      "209 batch_normalization_436\n",
      "210 activation_431\n",
      "211 activation_433\n",
      "212 activation_436\n",
      "213 block35_8_mixed\n",
      "214 block35_8_conv\n",
      "215 block35_8\n",
      "216 block35_8_ac\n",
      "217 conv2d_440\n",
      "218 batch_normalization_440\n",
      "219 activation_440\n",
      "220 conv2d_438\n",
      "221 conv2d_441\n",
      "222 batch_normalization_438\n",
      "223 batch_normalization_441\n",
      "224 activation_438\n",
      "225 activation_441\n",
      "226 conv2d_437\n",
      "227 conv2d_439\n",
      "228 conv2d_442\n",
      "229 batch_normalization_437\n",
      "230 batch_normalization_439\n",
      "231 batch_normalization_442\n",
      "232 activation_437\n",
      "233 activation_439\n",
      "234 activation_442\n",
      "235 block35_9_mixed\n",
      "236 block35_9_conv\n",
      "237 block35_9\n",
      "238 block35_9_ac\n",
      "239 conv2d_446\n",
      "240 batch_normalization_446\n",
      "241 activation_446\n",
      "242 conv2d_444\n",
      "243 conv2d_447\n",
      "244 batch_normalization_444\n",
      "245 batch_normalization_447\n",
      "246 activation_444\n",
      "247 activation_447\n",
      "248 conv2d_443\n",
      "249 conv2d_445\n",
      "250 conv2d_448\n",
      "251 batch_normalization_443\n",
      "252 batch_normalization_445\n",
      "253 batch_normalization_448\n",
      "254 activation_443\n",
      "255 activation_445\n",
      "256 activation_448\n",
      "257 block35_10_mixed\n",
      "258 block35_10_conv\n",
      "259 block35_10\n",
      "260 block35_10_ac\n",
      "261 conv2d_450\n",
      "262 batch_normalization_450\n",
      "263 activation_450\n",
      "264 conv2d_451\n",
      "265 batch_normalization_451\n",
      "266 activation_451\n",
      "267 conv2d_449\n",
      "268 conv2d_452\n",
      "269 batch_normalization_449\n",
      "270 batch_normalization_452\n",
      "271 activation_449\n",
      "272 activation_452\n",
      "273 max_pooling2d_19\n",
      "274 mixed_6a\n",
      "275 conv2d_454\n",
      "276 batch_normalization_454\n",
      "277 activation_454\n",
      "278 conv2d_455\n",
      "279 batch_normalization_455\n",
      "280 activation_455\n",
      "281 conv2d_453\n",
      "282 conv2d_456\n",
      "283 batch_normalization_453\n",
      "284 batch_normalization_456\n",
      "285 activation_453\n",
      "286 activation_456\n",
      "287 block17_1_mixed\n",
      "288 block17_1_conv\n",
      "289 block17_1\n",
      "290 block17_1_ac\n",
      "291 conv2d_458\n",
      "292 batch_normalization_458\n",
      "293 activation_458\n",
      "294 conv2d_459\n",
      "295 batch_normalization_459\n",
      "296 activation_459\n",
      "297 conv2d_457\n",
      "298 conv2d_460\n",
      "299 batch_normalization_457\n",
      "300 batch_normalization_460\n",
      "301 activation_457\n",
      "302 activation_460\n",
      "303 block17_2_mixed\n",
      "304 block17_2_conv\n",
      "305 block17_2\n",
      "306 block17_2_ac\n",
      "307 conv2d_462\n",
      "308 batch_normalization_462\n",
      "309 activation_462\n",
      "310 conv2d_463\n",
      "311 batch_normalization_463\n",
      "312 activation_463\n",
      "313 conv2d_461\n",
      "314 conv2d_464\n",
      "315 batch_normalization_461\n",
      "316 batch_normalization_464\n",
      "317 activation_461\n",
      "318 activation_464\n",
      "319 block17_3_mixed\n",
      "320 block17_3_conv\n",
      "321 block17_3\n",
      "322 block17_3_ac\n",
      "323 conv2d_466\n",
      "324 batch_normalization_466\n",
      "325 activation_466\n",
      "326 conv2d_467\n",
      "327 batch_normalization_467\n",
      "328 activation_467\n",
      "329 conv2d_465\n",
      "330 conv2d_468\n",
      "331 batch_normalization_465\n",
      "332 batch_normalization_468\n",
      "333 activation_465\n",
      "334 activation_468\n",
      "335 block17_4_mixed\n",
      "336 block17_4_conv\n",
      "337 block17_4\n",
      "338 block17_4_ac\n",
      "339 conv2d_470\n",
      "340 batch_normalization_470\n",
      "341 activation_470\n",
      "342 conv2d_471\n",
      "343 batch_normalization_471\n",
      "344 activation_471\n",
      "345 conv2d_469\n",
      "346 conv2d_472\n",
      "347 batch_normalization_469\n",
      "348 batch_normalization_472\n",
      "349 activation_469\n",
      "350 activation_472\n",
      "351 block17_5_mixed\n",
      "352 block17_5_conv\n",
      "353 block17_5\n",
      "354 block17_5_ac\n",
      "355 conv2d_474\n",
      "356 batch_normalization_474\n",
      "357 activation_474\n",
      "358 conv2d_475\n",
      "359 batch_normalization_475\n",
      "360 activation_475\n",
      "361 conv2d_473\n",
      "362 conv2d_476\n",
      "363 batch_normalization_473\n",
      "364 batch_normalization_476\n",
      "365 activation_473\n",
      "366 activation_476\n",
      "367 block17_6_mixed\n",
      "368 block17_6_conv\n",
      "369 block17_6\n",
      "370 block17_6_ac\n",
      "371 conv2d_478\n",
      "372 batch_normalization_478\n",
      "373 activation_478\n",
      "374 conv2d_479\n",
      "375 batch_normalization_479\n",
      "376 activation_479\n",
      "377 conv2d_477\n",
      "378 conv2d_480\n",
      "379 batch_normalization_477\n",
      "380 batch_normalization_480\n",
      "381 activation_477\n",
      "382 activation_480\n",
      "383 block17_7_mixed\n",
      "384 block17_7_conv\n",
      "385 block17_7\n",
      "386 block17_7_ac\n",
      "387 conv2d_482\n",
      "388 batch_normalization_482\n",
      "389 activation_482\n",
      "390 conv2d_483\n",
      "391 batch_normalization_483\n",
      "392 activation_483\n",
      "393 conv2d_481\n",
      "394 conv2d_484\n",
      "395 batch_normalization_481\n",
      "396 batch_normalization_484\n",
      "397 activation_481\n",
      "398 activation_484\n",
      "399 block17_8_mixed\n",
      "400 block17_8_conv\n",
      "401 block17_8\n",
      "402 block17_8_ac\n",
      "403 conv2d_486\n",
      "404 batch_normalization_486\n",
      "405 activation_486\n",
      "406 conv2d_487\n",
      "407 batch_normalization_487\n",
      "408 activation_487\n",
      "409 conv2d_485\n",
      "410 conv2d_488\n",
      "411 batch_normalization_485\n",
      "412 batch_normalization_488\n",
      "413 activation_485\n",
      "414 activation_488\n",
      "415 block17_9_mixed\n",
      "416 block17_9_conv\n",
      "417 block17_9\n",
      "418 block17_9_ac\n",
      "419 conv2d_490\n",
      "420 batch_normalization_490\n",
      "421 activation_490\n",
      "422 conv2d_491\n",
      "423 batch_normalization_491\n",
      "424 activation_491\n",
      "425 conv2d_489\n",
      "426 conv2d_492\n",
      "427 batch_normalization_489\n",
      "428 batch_normalization_492\n",
      "429 activation_489\n",
      "430 activation_492\n",
      "431 block17_10_mixed\n",
      "432 block17_10_conv\n",
      "433 block17_10\n",
      "434 block17_10_ac\n",
      "435 conv2d_494\n",
      "436 batch_normalization_494\n",
      "437 activation_494\n",
      "438 conv2d_495\n",
      "439 batch_normalization_495\n",
      "440 activation_495\n",
      "441 conv2d_493\n",
      "442 conv2d_496\n",
      "443 batch_normalization_493\n",
      "444 batch_normalization_496\n",
      "445 activation_493\n",
      "446 activation_496\n",
      "447 block17_11_mixed\n",
      "448 block17_11_conv\n",
      "449 block17_11\n",
      "450 block17_11_ac\n",
      "451 conv2d_498\n",
      "452 batch_normalization_498\n",
      "453 activation_498\n",
      "454 conv2d_499\n",
      "455 batch_normalization_499\n",
      "456 activation_499\n",
      "457 conv2d_497\n",
      "458 conv2d_500\n",
      "459 batch_normalization_497\n",
      "460 batch_normalization_500\n",
      "461 activation_497\n",
      "462 activation_500\n",
      "463 block17_12_mixed\n",
      "464 block17_12_conv\n",
      "465 block17_12\n",
      "466 block17_12_ac\n",
      "467 conv2d_502\n",
      "468 batch_normalization_502\n",
      "469 activation_502\n",
      "470 conv2d_503\n",
      "471 batch_normalization_503\n",
      "472 activation_503\n",
      "473 conv2d_501\n",
      "474 conv2d_504\n",
      "475 batch_normalization_501\n",
      "476 batch_normalization_504\n",
      "477 activation_501\n",
      "478 activation_504\n",
      "479 block17_13_mixed\n",
      "480 block17_13_conv\n",
      "481 block17_13\n",
      "482 block17_13_ac\n",
      "483 conv2d_506\n",
      "484 batch_normalization_506\n",
      "485 activation_506\n",
      "486 conv2d_507\n",
      "487 batch_normalization_507\n",
      "488 activation_507\n",
      "489 conv2d_505\n",
      "490 conv2d_508\n",
      "491 batch_normalization_505\n",
      "492 batch_normalization_508\n",
      "493 activation_505\n",
      "494 activation_508\n",
      "495 block17_14_mixed\n",
      "496 block17_14_conv\n",
      "497 block17_14\n",
      "498 block17_14_ac\n",
      "499 conv2d_510\n",
      "500 batch_normalization_510\n",
      "501 activation_510\n",
      "502 conv2d_511\n",
      "503 batch_normalization_511\n",
      "504 activation_511\n",
      "505 conv2d_509\n",
      "506 conv2d_512\n",
      "507 batch_normalization_509\n",
      "508 batch_normalization_512\n",
      "509 activation_509\n",
      "510 activation_512\n",
      "511 block17_15_mixed\n",
      "512 block17_15_conv\n",
      "513 block17_15\n",
      "514 block17_15_ac\n",
      "515 conv2d_514\n",
      "516 batch_normalization_514\n",
      "517 activation_514\n",
      "518 conv2d_515\n",
      "519 batch_normalization_515\n",
      "520 activation_515\n",
      "521 conv2d_513\n",
      "522 conv2d_516\n",
      "523 batch_normalization_513\n",
      "524 batch_normalization_516\n",
      "525 activation_513\n",
      "526 activation_516\n",
      "527 block17_16_mixed\n",
      "528 block17_16_conv\n",
      "529 block17_16\n",
      "530 block17_16_ac\n",
      "531 conv2d_518\n",
      "532 batch_normalization_518\n",
      "533 activation_518\n",
      "534 conv2d_519\n",
      "535 batch_normalization_519\n",
      "536 activation_519\n",
      "537 conv2d_517\n",
      "538 conv2d_520\n",
      "539 batch_normalization_517\n",
      "540 batch_normalization_520\n",
      "541 activation_517\n",
      "542 activation_520\n",
      "543 block17_17_mixed\n",
      "544 block17_17_conv\n",
      "545 block17_17\n",
      "546 block17_17_ac\n",
      "547 conv2d_522\n",
      "548 batch_normalization_522\n",
      "549 activation_522\n",
      "550 conv2d_523\n",
      "551 batch_normalization_523\n",
      "552 activation_523\n",
      "553 conv2d_521\n",
      "554 conv2d_524\n",
      "555 batch_normalization_521\n",
      "556 batch_normalization_524\n",
      "557 activation_521\n",
      "558 activation_524\n",
      "559 block17_18_mixed\n",
      "560 block17_18_conv\n",
      "561 block17_18\n",
      "562 block17_18_ac\n",
      "563 conv2d_526\n",
      "564 batch_normalization_526\n",
      "565 activation_526\n",
      "566 conv2d_527\n",
      "567 batch_normalization_527\n",
      "568 activation_527\n",
      "569 conv2d_525\n",
      "570 conv2d_528\n",
      "571 batch_normalization_525\n",
      "572 batch_normalization_528\n",
      "573 activation_525\n",
      "574 activation_528\n",
      "575 block17_19_mixed\n",
      "576 block17_19_conv\n",
      "577 block17_19\n",
      "578 block17_19_ac\n",
      "579 conv2d_530\n",
      "580 batch_normalization_530\n",
      "581 activation_530\n",
      "582 conv2d_531\n",
      "583 batch_normalization_531\n",
      "584 activation_531\n",
      "585 conv2d_529\n",
      "586 conv2d_532\n",
      "587 batch_normalization_529\n",
      "588 batch_normalization_532\n",
      "589 activation_529\n",
      "590 activation_532\n",
      "591 block17_20_mixed\n",
      "592 block17_20_conv\n",
      "593 block17_20\n",
      "594 block17_20_ac\n",
      "595 conv2d_537\n",
      "596 batch_normalization_537\n",
      "597 activation_537\n",
      "598 conv2d_533\n",
      "599 conv2d_535\n",
      "600 conv2d_538\n",
      "601 batch_normalization_533\n",
      "602 batch_normalization_535\n",
      "603 batch_normalization_538\n",
      "604 activation_533\n",
      "605 activation_535\n",
      "606 activation_538\n",
      "607 conv2d_534\n",
      "608 conv2d_536\n",
      "609 conv2d_539\n",
      "610 batch_normalization_534\n",
      "611 batch_normalization_536\n",
      "612 batch_normalization_539\n",
      "613 activation_534\n",
      "614 activation_536\n",
      "615 activation_539\n",
      "616 max_pooling2d_20\n",
      "617 mixed_7a\n",
      "618 conv2d_541\n",
      "619 batch_normalization_541\n",
      "620 activation_541\n",
      "621 conv2d_542\n",
      "622 batch_normalization_542\n",
      "623 activation_542\n",
      "624 conv2d_540\n",
      "625 conv2d_543\n",
      "626 batch_normalization_540\n",
      "627 batch_normalization_543\n",
      "628 activation_540\n",
      "629 activation_543\n",
      "630 block8_1_mixed\n",
      "631 block8_1_conv\n",
      "632 block8_1\n",
      "633 block8_1_ac\n",
      "634 conv2d_545\n",
      "635 batch_normalization_545\n",
      "636 activation_545\n",
      "637 conv2d_546\n",
      "638 batch_normalization_546\n",
      "639 activation_546\n",
      "640 conv2d_544\n",
      "641 conv2d_547\n",
      "642 batch_normalization_544\n",
      "643 batch_normalization_547\n",
      "644 activation_544\n",
      "645 activation_547\n",
      "646 block8_2_mixed\n",
      "647 block8_2_conv\n",
      "648 block8_2\n",
      "649 block8_2_ac\n",
      "650 conv2d_549\n",
      "651 batch_normalization_549\n",
      "652 activation_549\n",
      "653 conv2d_550\n",
      "654 batch_normalization_550\n",
      "655 activation_550\n",
      "656 conv2d_548\n",
      "657 conv2d_551\n",
      "658 batch_normalization_548\n",
      "659 batch_normalization_551\n",
      "660 activation_548\n",
      "661 activation_551\n",
      "662 block8_3_mixed\n",
      "663 block8_3_conv\n",
      "664 block8_3\n",
      "665 block8_3_ac\n",
      "666 conv2d_553\n",
      "667 batch_normalization_553\n",
      "668 activation_553\n",
      "669 conv2d_554\n",
      "670 batch_normalization_554\n",
      "671 activation_554\n",
      "672 conv2d_552\n",
      "673 conv2d_555\n",
      "674 batch_normalization_552\n",
      "675 batch_normalization_555\n",
      "676 activation_552\n",
      "677 activation_555\n",
      "678 block8_4_mixed\n",
      "679 block8_4_conv\n",
      "680 block8_4\n",
      "681 block8_4_ac\n",
      "682 conv2d_557\n",
      "683 batch_normalization_557\n",
      "684 activation_557\n",
      "685 conv2d_558\n",
      "686 batch_normalization_558\n",
      "687 activation_558\n",
      "688 conv2d_556\n",
      "689 conv2d_559\n",
      "690 batch_normalization_556\n",
      "691 batch_normalization_559\n",
      "692 activation_556\n",
      "693 activation_559\n",
      "694 block8_5_mixed\n",
      "695 block8_5_conv\n",
      "696 block8_5\n",
      "697 block8_5_ac\n",
      "698 conv2d_561\n",
      "699 batch_normalization_561\n",
      "700 activation_561\n",
      "701 conv2d_562\n",
      "702 batch_normalization_562\n",
      "703 activation_562\n",
      "704 conv2d_560\n",
      "705 conv2d_563\n",
      "706 batch_normalization_560\n",
      "707 batch_normalization_563\n",
      "708 activation_560\n",
      "709 activation_563\n",
      "710 block8_6_mixed\n",
      "711 block8_6_conv\n",
      "712 block8_6\n",
      "713 block8_6_ac\n",
      "714 conv2d_565\n",
      "715 batch_normalization_565\n",
      "716 activation_565\n",
      "717 conv2d_566\n",
      "718 batch_normalization_566\n",
      "719 activation_566\n",
      "720 conv2d_564\n",
      "721 conv2d_567\n",
      "722 batch_normalization_564\n",
      "723 batch_normalization_567\n",
      "724 activation_564\n",
      "725 activation_567\n",
      "726 block8_7_mixed\n",
      "727 block8_7_conv\n",
      "728 block8_7\n",
      "729 block8_7_ac\n",
      "730 conv2d_569\n",
      "731 batch_normalization_569\n",
      "732 activation_569\n",
      "733 conv2d_570\n",
      "734 batch_normalization_570\n",
      "735 activation_570\n",
      "736 conv2d_568\n",
      "737 conv2d_571\n",
      "738 batch_normalization_568\n",
      "739 batch_normalization_571\n",
      "740 activation_568\n",
      "741 activation_571\n",
      "742 block8_8_mixed\n",
      "743 block8_8_conv\n",
      "744 block8_8\n",
      "745 block8_8_ac\n",
      "746 conv2d_573\n",
      "747 batch_normalization_573\n",
      "748 activation_573\n",
      "749 conv2d_574\n",
      "750 batch_normalization_574\n",
      "751 activation_574\n",
      "752 conv2d_572\n",
      "753 conv2d_575\n",
      "754 batch_normalization_572\n",
      "755 batch_normalization_575\n",
      "756 activation_572\n",
      "757 activation_575\n",
      "758 block8_9_mixed\n",
      "759 block8_9_conv\n",
      "760 block8_9\n",
      "761 block8_9_ac\n",
      "762 conv2d_577\n",
      "763 batch_normalization_577\n",
      "764 activation_577\n",
      "765 conv2d_578\n",
      "766 batch_normalization_578\n",
      "767 activation_578\n",
      "768 conv2d_576\n",
      "769 conv2d_579\n",
      "770 batch_normalization_576\n",
      "771 batch_normalization_579\n",
      "772 activation_576\n",
      "773 activation_579\n",
      "774 block8_10_mixed\n",
      "775 block8_10_conv\n",
      "776 block8_10\n",
      "777 conv_7b\n",
      "778 conv_7b_bn\n",
      "779 conv_7b_ac\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "966/966 [==============================] - 77s 80ms/step - loss: 1.1438 - acc: 0.6201\n",
      "Epoch 2/5\n",
      "966/966 [==============================] - 55s 57ms/step - loss: 0.6576 - acc: 0.7816\n",
      "Epoch 3/5\n",
      "966/966 [==============================] - 57s 59ms/step - loss: 0.4937 - acc: 0.8292\n",
      "Epoch 4/5\n",
      "966/966 [==============================] - 56s 58ms/step - loss: 0.4580 - acc: 0.8520\n",
      "Epoch 5/5\n",
      "966/966 [==============================] - 55s 57ms/step - loss: 0.3408 - acc: 0.8903\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f11758a4f98>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from inception V3. We will freeze the bottom N layers\n",
    "# and train the remaining top layers.\n",
    "\n",
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "   print(i, layer.name)\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in model.layers[:249]:\n",
    "   layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "   layer.trainable = True\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "from keras.optimizers import SGD\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#model.compile(optimizer=Adagrad(lr=0.01, decay=0.0), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322/322 [==============================] - 13s 39ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[15.06691505005641, 0.06521739130434782]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
